[package]
name = "candle_pipelines"
description = "Simple, intuitive pipelines for local LLM inference in Rust, powered by Candle. Inspired by Python's Transformers library."
version = "0.0.1"
edition = "2021"
license = "MIT OR Apache-2.0" 
repository = "https://github.com/ljt019/candle_pipelines/"

[dependencies]
minijinja = { version = "2.9.0", features = ["json", "loader"] }
serde = { version = "1.0.219", features = ["derive"] }
tokenizers = "0.21.1"
serde_json = "1.0"
schemars = { version = "0.8", features = ["derive"] }
tracing = "0.1.41"
rand = "0.9.1"
candle_pipelines_macros = { path = "../candle_pipelines_macros", version = "0.0.1" }
once_cell = "1.20"
regex = "1"
futures = "0.3.31"
async-stream = "0.3.6"
thiserror = "2.0.12"
jsonschema = "0.18"
minijinja-contrib = { version = "2.9.0", features = ["pycompat"] }
aho-corasick = "1.1.3"
candle-core = { git = "https://github.com/huggingface/candle.git", rev = "8839457c70316c7e2dd31aca66b264d7b9d21e91" }
candle-nn = { git = "https://github.com/huggingface/candle.git", rev = "8839457c70316c7e2dd31aca66b264d7b9d21e91" }
candle-transformers = { git = "https://github.com/huggingface/candle.git", rev = "8839457c70316c7e2dd31aca66b264d7b9d21e91" }
tokio = { version = "1", features = ["full"] }
pin-project-lite = "0.2"
hf-hub = { version = "0.4.3", features = ["tokio"] }

[features]
default = []
cuda = ["candle-core/cuda", "candle-nn/cuda", "candle-transformers/cuda"]
metal = ["candle-core/metal", "candle-nn/metal", "candle-transformers/metal"]

[dev-dependencies]
